{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54cfa5b8-67f1-4bd8-a0a2-7dfc139133c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0cc0a85-8cad-4e8a-bd80-4c8bc9319822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/Desktop/SuperteamsAI/News aggregator/Reseach'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e8626a-dc2e-4844-b10c-60370aabef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f89d874-f04b-4797-9ad2-24e67ab3129b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/Desktop/SuperteamsAI/News aggregator'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1d3d626-e6b0-4f4e-87be-7ecd63927652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "logging.basicConfig(\n",
    "    # filename='extract_data.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%dÂ %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a58ae5-e03e-4d68-9887-2924f412def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from crewai import Agent, Task, Crew\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"NA\"\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model = \"crewai-llama3\",\n",
    "#     base_url = \"http://localhost:11434/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d87eb-01ac-406e-8c53-8d1bdcc8a022",
   "metadata": {},
   "source": [
    "# Content Planner Agent & Create planner task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64aa21c1-20f7-45ae-8c81-0cb285b8c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from crewai import Agent, Task, Crew\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# from newspaper import Article\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Set up the OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"NA\"\n",
    "\n",
    "# # Set up the LLM for writer and editor\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"crewai-llama3\",\n",
    "#     base_url=\"http://localhost:11434/v1\"\n",
    "# )\n",
    "\n",
    "# # Function to perform a Google search and return search results\n",
    "# def google_search(query):\n",
    "#     search_url = f\"https://www.google.com/search?q={query}\"\n",
    "#     headers = {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "#     }\n",
    "#     response = requests.get(search_url, headers=headers)\n",
    "#     response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#     search_results = []\n",
    "\n",
    "#     for item in soup.select('div.g'):\n",
    "#         title = item.select_one('h3')\n",
    "#         link = item.select_one('a')['href']\n",
    "#         if title and link:\n",
    "#             search_results.append({\n",
    "#                 \"title\": title.get_text(),\n",
    "#                 \"link\": link\n",
    "#             })\n",
    "    \n",
    "#     return search_results\n",
    "\n",
    "# # Function to extract article attributes\n",
    "# def extract_article_attributes(url):\n",
    "#     try:\n",
    "#         article = Article(url)\n",
    "#         article.download()\n",
    "#         article.parse()\n",
    "#         return {\n",
    "#             'authors': article.authors,\n",
    "#             'text': article.text,\n",
    "#             'title': article.title,\n",
    "#             'link': url\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to process {url}: {e}\")\n",
    "#         return {\n",
    "#             'authors': None,\n",
    "#             'text': None,\n",
    "#             'title': None,\n",
    "#             'link': url\n",
    "#         }\n",
    "\n",
    "# # Function to perform a search, store results in a DataFrame, and extract article attributes\n",
    "# def search_and_store_to_dataframe(query, filename=None):\n",
    "#     results = google_search(query)\n",
    "#     df = pd.DataFrame(results)\n",
    "    \n",
    "#     if filename:\n",
    "#         df.to_csv(filename, index=False)  # Save initial search results to CSV file\n",
    "\n",
    "#     # Extract article attributes\n",
    "#     attributes_df = df['link'].apply(extract_article_attributes).apply(pd.Series)\n",
    "#     df = pd.concat([df, attributes_df], axis=1)\n",
    "\n",
    "#     # Drop unnecessary columns and keep only 'link', 'authors', 'text', and 'title'\n",
    "#     df = df[['link', 'authors', 'text', 'title']]\n",
    "    \n",
    "#     if filename:\n",
    "#         filename = Path(filename).stem + \"_with_attributes.csv\"\n",
    "#         df.to_csv(filename, index=False)  # Save DataFrame with attributes to CSV file\n",
    "\n",
    "#     print(df)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Function to plan content\n",
    "# def plan_content(topic):\n",
    "#     query = topic\n",
    "#     filename = \"Dataset/search_results.csv\"\n",
    "#     df = search_and_store_to_dataframe(query, filename)\n",
    "\n",
    "#     # Extract the necessary details for the content plan\n",
    "#     latest_trends = df.head(5)  # Example of prioritizing the latest trends\n",
    "#     target_audience = \"General readers interested in the topic\"\n",
    "#     seo_keywords = [\"example keyword1\", \"example keyword2\"]  # These would be derived from the analysis\n",
    "#     content_outline = {\n",
    "#         \"Introduction\": \"Brief introduction to the topic.\",\n",
    "#         \"Key Points\": latest_trends.to_dict('records'),\n",
    "#         \"Conclusion\": \"Summary and call to action.\"\n",
    "#     }\n",
    "\n",
    "#     content_plan = {\n",
    "#         \"Topic\": topic,\n",
    "#         \"Target Audience\": target_audience,\n",
    "#         \"SEO Keywords\": seo_keywords,\n",
    "#         \"Content Outline\": content_outline,\n",
    "#         \"Resources\": latest_trends.to_dict('records')\n",
    "#     }\n",
    "\n",
    "#     return content_plan\n",
    "\n",
    "# # Mock LLM class to simulate the planner agent's behavior\n",
    "# class MockLLM:\n",
    "#     def bind(self, *args, **kwargs):\n",
    "#         def call(inputs):\n",
    "#             return plan_content(inputs)\n",
    "#         return call\n",
    "\n",
    "# # Define the agents\n",
    "# planner = Agent(\n",
    "#     role=\"Content Planner\",\n",
    "#     goal=\"Plan engaging and factually accurate content on {topic}\",\n",
    "#     backstory=\"You're working on planning a blog article about the topic: {topic} on 'https://medium.com/'.\"\n",
    "#               \" You collect information that helps the audience learn something and make informed decisions.\"\n",
    "#               \" You have to prepare a detailed outline and the relevant topics and sub-topics that have to be part of the blog post.\"\n",
    "#               \" Your work is the basis for the Content Writer to write an article on this topic.\",\n",
    "#     llm=MockLLM(),\n",
    "#     allow_delegation=False,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# # Define the tasks for each agent\n",
    "# plan_task = Task(\n",
    "#     description=(\n",
    "#         \"1. Perform a Google search to find the latest trends, key players, and noteworthy news on {topic}.\\n\"\n",
    "#         \"2. Extract article attributes such as authors and main content.\\n\"\n",
    "#         \"3. Develop a detailed content outline including an introduction, key points, and a call to action.\\n\"\n",
    "#         \"4. Identify the target audience and include SEO keywords and relevant data or sources.\"\n",
    "#     ),\n",
    "#     expected_output=\"A comprehensive content plan document with an outline, audience analysis, SEO keywords, and resources.\",\n",
    "#     agent=planner,\n",
    "#     action=lambda inputs: planner.llm.bind()(inputs[\"topic\"])\n",
    "# )\n",
    "\n",
    "# writer = Agent(\n",
    "#     role=\"Content Writer\",\n",
    "#     goal=\"Write insightful and factually accurate opinion piece about the topic: {topic}\",\n",
    "#     backstory=\"You're working on a writing a new opinion piece about the topic: {topic} in 'https://medium.com/'. \"\n",
    "#               \"You base your writing on the work of the Content Planner, who provides an outline and relevant context about the topic. \"\n",
    "#               \"You follow the main objectives and direction of the outline, as provide by the Content Planner. \"\n",
    "#               \"You also provide objective and impartial insights and back them up with information provide by the Content Planner. \"\n",
    "#               \"You acknowledge in your opinion piece when your statements are opinions as opposed to objective statements.\",\n",
    "#     allow_delegation=False,\n",
    "#     llm=llm,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# editor = Agent(\n",
    "#     role=\"Editor\",\n",
    "#     goal=\"Edit and refine the article to ensure clarity, accuracy, and engagement\",\n",
    "#     backstory=\"You are an editor responsible for polishing the article to make it publish-ready.\",\n",
    "#     llm=llm,\n",
    "#     allow_delegation=False,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# write_task = Task(\n",
    "#     description=(\n",
    "#         \"1. Use the content plan to craft a compelling blog post on {topic}.\\n\"\n",
    "#         \"2. Incorporate SEO keywords naturally.\\n\"\n",
    "#         \"3. Sections/Subtitles are properly named in an engaging manner.\\n\"\n",
    "#         \"4. Ensure the post is structured with an engaging introduction, insightful body, and a summarizing conclusion.\\n\"\n",
    "#         \"5. Proofread for grammatical errors and alignment with the brand's voice.\\n\"\n",
    "#     ),\n",
    "#     expected_output=\"A well-written blog post in markdown format, ready for publication, each section should have 2 or 3 paragraphs.\",\n",
    "#     agent=writer,\n",
    "# )\n",
    "\n",
    "# edit_task = Task(\n",
    "#     description=(\"Proofread the given blog post for grammatical errors and alignment with the brand's voice.\"),\n",
    "#     expected_output=\"A well-written blog post in markdown format, ready for publication, each section should have 2 or 3 paragraphs.\",\n",
    "#     agent=editor\n",
    "# )\n",
    "\n",
    "# # Crew setup with agents and tasks\n",
    "# crew = Crew(\n",
    "#     agents=[planner, writer, editor],\n",
    "#     tasks=[plan_task, write_task, edit_task],\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Example function call for the content planner agent\n",
    "# topic = \"Modi takes office\"\n",
    "# inputs = {\"topic\": topic}\n",
    "# result = crew.kickoff(inputs=inputs)\n",
    "\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "460d347f-015c-41c7-a175-27364ec7f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mock LLM class to simulate the planner agent's behavior\n",
    "# class MockLLM:\n",
    "#     def bind(self, *args, **kwargs):\n",
    "#         def call(inputs):\n",
    "#             return plan_content(inputs)\n",
    "#         return call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ec30a-727f-4351-90a0-e92fbe85174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from crewai import Agent, Task, Crew\n",
    "from langchain_openai import ChatOpenAI\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"NA\"\n",
    "\n",
    "# Set up the LLM for writer and editor\n",
    "llm = ChatOpenAI(\n",
    "    model=\"crewai-llama3\",\n",
    "    base_url=\"http://localhost:11434/v1\"\n",
    ")\n",
    "\n",
    "# Function to perform a Google search and return search results\n",
    "def google_search(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    search_results = []\n",
    "    max_retries = 3\n",
    "    retry_delay = 2\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(search_url, headers=headers)\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            for item in soup.select('div.g'):\n",
    "                title = item.select_one('h3')\n",
    "                link = item.select_one('a')['href']\n",
    "                if title and link:\n",
    "                    search_results.append({\n",
    "                        \"title\": title.get_text(),\n",
    "                        \"link\": link\n",
    "                    })\n",
    "            break\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 429:  # Too Many Requests\n",
    "                retries += 1\n",
    "                print(f\"Rate limit hit. Waiting for {retry_delay} seconds before retrying... (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    if retries == max_retries:\n",
    "        print(\"Max retries reached. Failed to retrieve search results.\")\n",
    "    \n",
    "    return search_results\n",
    "\n",
    "# Function to extract article attributes\n",
    "def extract_article_attributes(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return {\n",
    "            'authors': article.authors,\n",
    "            'text': article.text,\n",
    "            'title': article.title,\n",
    "            'link': url\n",
    "        }\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 403:\n",
    "            print(f\"403 Forbidden: Skipping URL {url}\")\n",
    "        else:\n",
    "            print(f\"Failed to process {url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to perform a search, store results in a DataFrame, and extract article attributes\n",
    "def search_and_store_to_dataframe(query, filename=None):\n",
    "    results = google_search(query)\n",
    "    articles = [extract_article_attributes(result[\"link\"]) for result in results]\n",
    "    articles = [article for article in articles if article is not None]  # Filter out failed downloads\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    if filename:\n",
    "        df.to_csv(filename, index=False)  # Save DataFrame with attributes to CSV file\n",
    "\n",
    "    print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to plan content\n",
    "def plan_content(topic):\n",
    "    query = topic\n",
    "    filename = \"Dataset/search_results.csv\"\n",
    "    df = search_and_store_to_dataframe(query, filename)\n",
    "    # Drop rows with any NaN values\n",
    "    df_cleaned = df.dropna()\n",
    "    \n",
    "    # Display DataFrame after dropping NaN values\n",
    "    print(\"\\nDataFrame after dropping NaN values:\")\n",
    "    print(df_cleaned)\n",
    "    # Extract the necessary details for the content plan\n",
    "    latest_trends = df_cleaned.head(5)  # Example of prioritizing the latest trends\n",
    "    target_audience = \"General readers interested in the topic\"\n",
    "    seo_keywords = [\"example keyword1\", \"example keyword2\"]  # These would be derived from the analysis\n",
    "    content_outline = {\n",
    "        \"Introduction\": \"Brief introduction to the topic.\",\n",
    "        \"Key Points\": latest_trends.to_dict('records'),\n",
    "        \"Conclusion\": \"Summary and call to action.\"\n",
    "    }\n",
    "\n",
    "    content_plan = {\n",
    "        \"Topic\": topic,\n",
    "        \"Target Audience\": target_audience,\n",
    "        \"SEO Keywords\": seo_keywords,\n",
    "        \"Content Outline\": content_outline,\n",
    "        \"Resources\": latest_trends.to_dict('records')\n",
    "    }\n",
    "\n",
    "    # Convert content_plan to a string for return\n",
    "    content_plan_str = (\n",
    "        f\"Topic: {content_plan['Topic']}\\n\"\n",
    "        f\"Target Audience: {content_plan['Target Audience']}\\n\"\n",
    "        f\"SEO Keywords: {', '.join(content_plan['SEO Keywords'])}\\n\"\n",
    "        f\"Content Outline: \\n\"\n",
    "        f\"  Introduction: {content_outline['Introduction']}\\n\"\n",
    "        f\"  Key Points: {content_outline['Key Points']}\\n\"\n",
    "        f\"  Conclusion: {content_outline['Conclusion']}\\n\"\n",
    "        f\"Resources: {content_plan['Resources']}\"\n",
    "    )\n",
    "\n",
    "    return content_plan_str\n",
    "\n",
    "# Mock LLM class to simulate the planner agent's behavior\n",
    "class MockLLM:\n",
    "    def bind(self, *args, **kwargs):\n",
    "        def call(inputs):\n",
    "            return plan_content(inputs)\n",
    "        return call\n",
    "\n",
    "# Define the agents\n",
    "planner = Agent(\n",
    "    role=\"Content Planner\",\n",
    "    goal=\"Plan engaging and factually accurate content on {topic}\",\n",
    "    backstory=\"You're working on planning a blog article about the topic: {topic} on 'https://medium.com/'.\"\n",
    "              \" You collect information that helps the audience learn something and make informed decisions.\"\n",
    "              \" You have to prepare a detailed outline and the relevant topics and sub-topics that have to be part of the blog post.\"\n",
    "              \" Your work is the basis for the Content Writer to write an article on this topic.\",\n",
    "    llm=MockLLM(),\n",
    "    allow_delegation=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define the tasks for each agent\n",
    "plan_task = Task(\n",
    "    description=(\n",
    "        \"1. Perform a Google search to find the latest trends, key players, and noteworthy news on {topic}.\\n\"\n",
    "        \"2. Extract article attributes such as authors and main content.\\n\"\n",
    "        \"3. Develop a detailed content outline including an introduction, key points, and a call to action.\\n\"\n",
    "        \"4. Identify the target audience and include SEO keywords and relevant data or sources.\"\n",
    "    ),\n",
    "    expected_output=\"A comprehensive content plan document with an outline, audience analysis, SEO keywords, and resources.\",\n",
    "    agent=planner,\n",
    "    action=lambda inputs: planner.llm.bind()(inputs)\n",
    ")\n",
    "\n",
    "writer = Agent(\n",
    "    role=\"Content Writer\",\n",
    "    goal=\"Write insightful and factually accurate opinion piece about the topic: {topic}\",\n",
    "    backstory=\"You're working on a writing a new opinion piece about the topic: {topic} in 'https://medium.com/'. \"\n",
    "              \"You base your writing on the work of the Content Planner, who provides an outline and relevant context about the topic. \"\n",
    "              \"You follow the main objectives and direction of the outline, as provide by the Content Planner. \"\n",
    "              \"You also provide objective and impartial insights and back them up with information provide by the Content Planner. \"\n",
    "              \"You acknowledge in your opinion piece when your statements are opinions as opposed to objective statements.\",\n",
    "    allow_delegation=False,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "editor = Agent(\n",
    "    role=\"Editor\",\n",
    "    goal=\"Edit and refine the article to ensure clarity, accuracy, and engagement\",\n",
    "    backstory=\"You are an editor responsible for polishing the article to make it publish-ready.\",\n",
    "    llm=llm,\n",
    "    allow_delegation=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "write_task = Task(\n",
    "    description=(\n",
    "        \"1. Use the content plan to craft a compelling blog post on {topic}.\\n\"\n",
    "        \"2. Incorporate SEO keywords naturally.\\n\"\n",
    "        \"3. Sections/Subtitles are properly named in an engaging manner.\\n\"\n",
    "        \"4. Ensure the post is structured with an engaging introduction, insightful body, and a summarizing conclusion.\\n\"\n",
    "        \"5. Proofread for grammatical errors and alignment with the brand's voice.\\n\"\n",
    "    ),\n",
    "    expected_output=\"A well-written blog post in markdown format, ready for publication, each section should have 2 or 3 paragraphs.\",\n",
    "    agent=writer,\n",
    ")\n",
    "\n",
    "edit_task = Task(\n",
    "    description=(\"Proofread the given blog post for grammatical errors and alignment with the brand's voice.\"),\n",
    "    expected_output=\"A well-written blog post in markdown format, ready for publication, each section should have 2 or 3 paragraphs.\",\n",
    "    agent=editor\n",
    ")\n",
    "\n",
    "# Crew setup with agents and tasks\n",
    "crew = Crew(\n",
    "    agents=[planner, writer, editor],\n",
    "    tasks=[plan_task, write_task, edit_task],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Example function call for the content planner agent\n",
    "topic = \"Modi takes office\"\n",
    "inputs = {\"topic\": topic}\n",
    "result = crew.kickoff(inputs=inputs)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00a454-a667-4f13-b9e2-9ff272f432ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown,display\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a34ea4-9e29-45bb-a1f9-febf24716a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
